{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementasi Metode VADER-LSTM dalam Pengujian Pengaruh Sentimen Investor terhadap Prediksi Harga Saham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljaywiikl4zP",
        "outputId": "49b92a99-8e67-4e03-e2b4-4c7eb7f33112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\ravie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ravie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ravie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import nltk\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.dates as dt\n",
        "from matplotlib.dates import DateFormatter\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PoB1vjA469iM"
      },
      "outputs": [],
      "source": [
        "company = \"TSLA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Tweet Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "s4gocCIkkUkY",
        "outputId": "e5a6b992-2edf-4d84-d7ee-5fa4f03d4a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80793, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stock Name</th>\n",
              "      <th>Company Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-09-29 23:41:16+00:00</td>\n",
              "      <td>Mainstream media has done an amazing job at br...</td>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla, Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-09-29 23:24:43+00:00</td>\n",
              "      <td>Tesla delivery estimates are at around 364k fr...</td>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla, Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-09-29 23:18:08+00:00</td>\n",
              "      <td>3/ Even if I include 63.0M unvested RSUs as of...</td>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla, Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-09-29 22:40:07+00:00</td>\n",
              "      <td>@RealDanODowd @WholeMarsBlog @Tesla Hahaha why...</td>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla, Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-09-29 22:27:05+00:00</td>\n",
              "      <td>@RealDanODowd @Tesla Stop trying to kill kids,...</td>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla, Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80788</th>\n",
              "      <td>2021-10-07 17:11:57+00:00</td>\n",
              "      <td>Some of the fastest growing tech stocks on the...</td>\n",
              "      <td>XPEV</td>\n",
              "      <td>XPeng Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80789</th>\n",
              "      <td>2021-10-04 17:05:59+00:00</td>\n",
              "      <td>With earnings on the horizon, here is a quick ...</td>\n",
              "      <td>XPEV</td>\n",
              "      <td>XPeng Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80790</th>\n",
              "      <td>2021-10-01 04:43:41+00:00</td>\n",
              "      <td>Our record delivery results are a testimony of...</td>\n",
              "      <td>XPEV</td>\n",
              "      <td>XPeng Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80791</th>\n",
              "      <td>2021-10-01 00:03:32+00:00</td>\n",
              "      <td>We delivered 10,412 Smart EVs in Sep 2021, rea...</td>\n",
              "      <td>XPEV</td>\n",
              "      <td>XPeng Inc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80792</th>\n",
              "      <td>2021-09-30 10:22:52+00:00</td>\n",
              "      <td>Why can XPeng P5 deliver outstanding performan...</td>\n",
              "      <td>XPEV</td>\n",
              "      <td>XPeng Inc.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80793 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Date  \\\n",
              "0      2022-09-29 23:41:16+00:00   \n",
              "1      2022-09-29 23:24:43+00:00   \n",
              "2      2022-09-29 23:18:08+00:00   \n",
              "3      2022-09-29 22:40:07+00:00   \n",
              "4      2022-09-29 22:27:05+00:00   \n",
              "...                          ...   \n",
              "80788  2021-10-07 17:11:57+00:00   \n",
              "80789  2021-10-04 17:05:59+00:00   \n",
              "80790  2021-10-01 04:43:41+00:00   \n",
              "80791  2021-10-01 00:03:32+00:00   \n",
              "80792  2021-09-30 10:22:52+00:00   \n",
              "\n",
              "                                                   Tweet Stock Name  \\\n",
              "0      Mainstream media has done an amazing job at br...       TSLA   \n",
              "1      Tesla delivery estimates are at around 364k fr...       TSLA   \n",
              "2      3/ Even if I include 63.0M unvested RSUs as of...       TSLA   \n",
              "3      @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...       TSLA   \n",
              "4      @RealDanODowd @Tesla Stop trying to kill kids,...       TSLA   \n",
              "...                                                  ...        ...   \n",
              "80788  Some of the fastest growing tech stocks on the...       XPEV   \n",
              "80789  With earnings on the horizon, here is a quick ...       XPEV   \n",
              "80790  Our record delivery results are a testimony of...       XPEV   \n",
              "80791  We delivered 10,412 Smart EVs in Sep 2021, rea...       XPEV   \n",
              "80792  Why can XPeng P5 deliver outstanding performan...       XPEV   \n",
              "\n",
              "      Company Name  \n",
              "0      Tesla, Inc.  \n",
              "1      Tesla, Inc.  \n",
              "2      Tesla, Inc.  \n",
              "3      Tesla, Inc.  \n",
              "4      Tesla, Inc.  \n",
              "...            ...  \n",
              "80788   XPeng Inc.  \n",
              "80789   XPeng Inc.  \n",
              "80790   XPeng Inc.  \n",
              "80791   XPeng Inc.  \n",
              "80792   XPeng Inc.  \n",
              "\n",
              "[80793 rows x 4 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tweets = pd.read_csv(\"stock_tweets.csv\")\n",
        "print(all_tweets.shape)\n",
        "all_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Hm4AXNZCk2el",
        "outputId": "319b5eda-637b-4401-be8a-d5843f2fb11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(37422, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>Mainstream media has done an amazing job at br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>Tesla delivery estimates are at around 364k fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>3/ Even if I include 63.0M unvested RSUs as of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>@RealDanODowd @WholeMarsBlog @Tesla Hahaha why...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>@RealDanODowd @Tesla Stop trying to kill kids,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date                                              Tweet\n",
              "0  2022-09-29  Mainstream media has done an amazing job at br...\n",
              "1  2022-09-29  Tesla delivery estimates are at around 364k fr...\n",
              "2  2022-09-29  3/ Even if I include 63.0M unvested RSUs as of...\n",
              "3  2022-09-29  @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...\n",
              "4  2022-09-29  @RealDanODowd @Tesla Stop trying to kill kids,..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_df = all_tweets[all_tweets['Stock Name'] == company]\n",
        "tweet_df = tweet_df.drop(['Company Name', 'Stock Name'], axis=1)\n",
        "tweet_df['Date'] = pd.to_datetime(tweet_df['Date'])\n",
        "tweet_df['Date'] = tweet_df['Date'].dt.date\n",
        "print(tweet_df.shape)\n",
        "tweet_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Labeling without pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unprocessed_df = tweet_df.copy()\n",
        "unprocessed_df[\"Compound\"] = pd.Series(dtype='float64')\n",
        "unprocessed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCBozioHs08B",
        "outputId": "b690b590-6ad9-4466-ac4b-0d1bfd90504e"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "for indx, row in unprocessed_df.T.items():\n",
        "    try:\n",
        "        sentence_i = unicodedata.normalize('NFKD', unprocessed_df.loc[indx, 'Tweet'])\n",
        "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
        "        unprocessed_df.at[indx, 'Compound'] = sentence_sentiment['compound']\n",
        "    except TypeError:\n",
        "        print (unprocessed_df.loc[indx, 'Tweet'])\n",
        "        print (indx)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unprocessed_df.to_csv('tweet_unprocessed_labeled.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unprocessed_df = pd.read_csv('tweet_unprocessed_labeled.csv')\n",
        "unprocessed_df['Date'] = pd.to_datetime(unprocessed_df['Date'])\n",
        "unprocessed_df['Date'] = unprocessed_df['Date'].dt.date\n",
        "unprocessed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alternate: Pre-process the tweets then labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>Mainstream media has done an amazing job at br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>Tesla delivery estimates are at around 364k fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>3/ Even if I include 63.0M unvested RSUs as of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>@RealDanODowd @WholeMarsBlog @Tesla Hahaha why...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>@RealDanODowd @Tesla Stop trying to kill kids,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37417</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>Playing in the dirt and #chasingsunsets\\n@tesl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37418</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>I agree with @freshjiva that $TSLA â€˜s EV busin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37419</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>Hold. On. Tight. $TSLA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37420</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>Get ready for a $TSLA _ _ _ _ _ _  Q3 delivery...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37421</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>In other words, AMD has been giving Tesla pref...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37422 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date                                              Tweet\n",
              "0      2022-09-29  Mainstream media has done an amazing job at br...\n",
              "1      2022-09-29  Tesla delivery estimates are at around 364k fr...\n",
              "2      2022-09-29  3/ Even if I include 63.0M unvested RSUs as of...\n",
              "3      2022-09-29  @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...\n",
              "4      2022-09-29  @RealDanODowd @Tesla Stop trying to kill kids,...\n",
              "...           ...                                                ...\n",
              "37417  2021-09-30  Playing in the dirt and #chasingsunsets\\n@tesl...\n",
              "37418  2021-09-30  I agree with @freshjiva that $TSLA â€˜s EV busin...\n",
              "37419  2021-09-30                             Hold. On. Tight. $TSLA\n",
              "37420  2021-09-30  Get ready for a $TSLA _ _ _ _ _ _  Q3 delivery...\n",
              "37421  2021-09-30  In other words, AMD has been giving Tesla pref...\n",
              "\n",
              "[37422 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df = tweet_df.copy()\n",
        "processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
        "    return text.replace(\"http://\", \" \".replace(\"https://\", \" \"))\n",
        "\n",
        "def removeStopword(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def stemming(text):\n",
        "    porter = PorterStemmer()\n",
        "    return porter.stem(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    '''\n",
        "    Takes a tweet as an input and output the list of tokens.\n",
        "    '''\n",
        "    \n",
        "    import emoji\n",
        "    import re\n",
        "    from nltk import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer\n",
        "    \n",
        "    # Initialization\n",
        "    new_tweet = tweet\n",
        "    \n",
        "    ## Changes on string\n",
        "    \n",
        "    # Remove urls\n",
        "    new_tweet = re.sub(r'https?://[^ ]+', '', new_tweet)\n",
        "    \n",
        "    # Remove usernames\n",
        "    new_tweet = re.sub(r'@[^ ]+', '', new_tweet)\n",
        "    \n",
        "    # Remove hashtags\n",
        "    new_tweet = re.sub(r'#', '', new_tweet)\n",
        "    \n",
        "    # Character normalization\n",
        "    new_tweet = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', new_tweet)\n",
        "    \n",
        "    # Emoji transformation\n",
        "    new_tweet = emoji.demojize(new_tweet)\n",
        "    \n",
        "    # Punctuation and special characters\n",
        "    new_tweet = re.sub(r' 0 ', 'zero', new_tweet)\n",
        "    new_tweet = re.sub(r'[^A-Za-z ]', '', new_tweet)\n",
        "    \n",
        "    # Lower casing\n",
        "    new_tweet = new_tweet.lower()\n",
        "    \n",
        "    \n",
        "    ## Changes on tokens\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(new_tweet)\n",
        "    \n",
        "    porter = PorterStemmer()\n",
        "    \n",
        "    for token in tokens:\n",
        "        # Stopwords removal\n",
        "        if token in stopwords.words('english'):\n",
        "            tokens.remove(token)\n",
        "        # Stemming\n",
        "            token = porter.stem(token)\n",
        "    \n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "for indx, row in processed_df.T.items():\n",
        "    try:\n",
        "        processed_df.at[indx, 'Tweet'] = preprocess_tweet(processed_df.at[indx, 'Tweet'])\n",
        "    except TypeError:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'encode'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ravie\\Documents\\Projects\\PythonProjects\\Skripsi\\VADER+LSTM copy 2.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ravie/Documents/Projects/PythonProjects/Skripsi/VADER%2BLSTM%20copy%202.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m indx, row \u001b[39min\u001b[39;00m processed_df\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ravie/Documents/Projects/PythonProjects/Skripsi/VADER%2BLSTM%20copy%202.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ravie/Documents/Projects/PythonProjects/Skripsi/VADER%2BLSTM%20copy%202.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         sentence_sentiment \u001b[39m=\u001b[39m sentiment_analyzer\u001b[39m.\u001b[39;49mpolarity_scores(processed_df\u001b[39m.\u001b[39;49mloc[indx, \u001b[39m'\u001b[39;49m\u001b[39mTweet\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ravie/Documents/Projects/PythonProjects/Skripsi/VADER%2BLSTM%20copy%202.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         processed_df\u001b[39m.\u001b[39mat[indx, \u001b[39m'\u001b[39m\u001b[39mCompound\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sentence_sentiment[\u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ravie/Documents/Projects/PythonProjects/Skripsi/VADER%2BLSTM%20copy%202.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ravie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\sentiment\\vader.py:366\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mReturn a float for sentiment strength based on the input text.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[39mPositive values are positive valence, negative value are negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m    matched as if it was a normal word in the sentence.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m sentitext \u001b[39m=\u001b[39m SentiText(\n\u001b[0;32m    367\u001b[0m     text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mPUNC_LIST, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mREGEX_REMOVE_PUNCTUATION\n\u001b[0;32m    368\u001b[0m )\n\u001b[0;32m    369\u001b[0m sentiments \u001b[39m=\u001b[39m []\n\u001b[0;32m    370\u001b[0m words_and_emoticons \u001b[39m=\u001b[39m sentitext\u001b[39m.\u001b[39mwords_and_emoticons\n",
            "File \u001b[1;32mc:\\Users\\ravie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\sentiment\\vader.py:270\u001b[0m, in \u001b[0;36mSentiText.__init__\u001b[1;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, text, punc_list, regex_remove_punctuation):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 270\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(text\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m text\n\u001b[0;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST \u001b[39m=\u001b[39m punc_list\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'encode'"
          ]
        }
      ],
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "for indx, row in processed_df.T.items():\n",
        "    try:\n",
        "        sentence_sentiment = sentiment_analyzer.polarity_scores(processed_df.loc[indx, 'Tweet'])\n",
        "        processed_df.at[indx, 'Compound'] = sentence_sentiment['compound']\n",
        "    except TypeError:\n",
        "        print (processed_df.loc[indx, 'Tweet'])\n",
        "        print (indx)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.to_csv('tweet_processed_labeled.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>mainstream media done amazing job brainwashing...</td>\n",
              "      <td>0.0772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>tesla delivery estimates at around k the analy...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>even i include unvested rsus of additional equ...</td>\n",
              "      <td>0.2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>hahaha are still trying stop tesla fsd bro get...</td>\n",
              "      <td>-0.7096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-09-29</td>\n",
              "      <td>stop trying kill kids sad deranged old man</td>\n",
              "      <td>-0.8750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37417</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>playing the dirt chasingsunsets</td>\n",
              "      <td>-0.1531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37418</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>agree tsla ev business alone worth gt sh wo fs...</td>\n",
              "      <td>0.7003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37419</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>hold tight tsla</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37420</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>get ready a tsla q delivery numberhave ur answ...</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37421</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>other words amd been giving tesla preferential...</td>\n",
              "      <td>0.3400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37422 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date                                              Tweet  Compound\n",
              "0      2022-09-29  mainstream media done amazing job brainwashing...    0.0772\n",
              "1      2022-09-29  tesla delivery estimates at around k the analy...    0.0000\n",
              "2      2022-09-29  even i include unvested rsus of additional equ...    0.2960\n",
              "3      2022-09-29  hahaha are still trying stop tesla fsd bro get...   -0.7096\n",
              "4      2022-09-29         stop trying kill kids sad deranged old man   -0.8750\n",
              "...           ...                                                ...       ...\n",
              "37417  2021-09-30                    playing the dirt chasingsunsets   -0.1531\n",
              "37418  2021-09-30  agree tsla ev business alone worth gt sh wo fs...    0.7003\n",
              "37419  2021-09-30                                    hold tight tsla    0.0000\n",
              "37420  2021-09-30  get ready a tsla q delivery numberhave ur answ...    0.3612\n",
              "37421  2021-09-30  other words amd been giving tesla preferential...    0.3400\n",
              "\n",
              "[37422 rows x 3 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df = pd.read_csv('tweet_processed_labeled.csv')\n",
        "processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
        "processed_df['Date'] = processed_df['Date'].dt.date\n",
        "processed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparing between unprocessed and pre-processed labeled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(processed_df.groupby([processed_df['Date']]).mean(numeric_only=True))\n",
        "ax.plot(unprocessed_df.groupby([unprocessed_df['Date']]).mean(numeric_only=True))\n",
        "ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Grouping sentiments by day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "gjvqS1Ijw7H3",
        "outputId": "f5a4c12e-03f7-4550-ba6f-4d5de8f1aa5f"
      },
      "outputs": [],
      "source": [
        "daily_sentiments_df = processed_df.groupby([processed_df['Date']]).mean(numeric_only=True)\n",
        "daily_sentiments_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "V8kKkLSulEMe",
        "outputId": "3fb9f070-4d05-41c7-f132-3395e3c37618"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(daily_sentiments_df['Compound'], color='#008B8B')\n",
        "ax.set(xlabel=\"Date\", ylabel=\"USD\", title=f\"{company} Daily Sentiment\")\n",
        "ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4S5DD9P95z9x",
        "outputId": "c3b7c868-417f-4961-de3a-33c060ab4e5d"
      },
      "outputs": [],
      "source": [
        "all_stocks = pd.read_csv(\"stock_yfinance_data.csv\")\n",
        "all_stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "L72o-YzF7zw7",
        "outputId": "675f77de-f30c-4760-b7d6-c09e08520ce8"
      },
      "outputs": [],
      "source": [
        "stock_df = all_stocks[all_stocks['Stock Name'] == company]\n",
        "stock_df = stock_df.drop('Stock Name', axis=1)\n",
        "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
        "stock_df['Date'] = stock_df['Date'].dt.date\n",
        "stock_df = stock_df.set_index(\"Date\")\n",
        "print(stock_df.shape)\n",
        "stock_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "K0pNKNfZCCKS",
        "outputId": "48e7c276-e5da-4e91-e616-627d072c8c12"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(stock_df['Close'], color='#008B8B')\n",
        "ax.set(xlabel=\"Date\", ylabel=\"USD\", title=f\"{company} Stock Price\")\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "QNbSzCPxogZP",
        "outputId": "a298b6b6-a8c8-4d01-f091-090cf6fbe619"
      },
      "outputs": [],
      "source": [
        "dataset_df = stock_df.copy()\n",
        "dataset_df = dataset_df.join(daily_sentiments_df, how=\"left\", on=\"Date\")\n",
        "print(dataset_df.shape)\n",
        "dataset_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SplitData(data, train_size, timestep):\n",
        "    training_data_len = math.ceil(len(data)* train_size)\n",
        "    \n",
        "    train_data = data[0: training_data_len, :]\n",
        "    test_data = data[training_data_len-timestep: , : ]\n",
        "\n",
        "    return train_data, test_data    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SplitDataNew(data, train_size, timestep):\n",
        "    data_values = data.values\n",
        "    training_data_len = math.ceil(len(data)* train_size)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    if (len(data_values.shape) == 1):\n",
        "        scaled_data = scaler.fit_transform(data_values.reshape(-1,1))\n",
        "    else:\n",
        "        scaled_data = scaler.fit_transform(data_values)\n",
        "        scaled_index = scaler.fit_transform(data_values[:, 0:1].flatten().reshape(-1,1))\n",
        "\n",
        "    train_data = scaled_data[0: training_data_len, :]\n",
        "    test_data = scaled_data[training_data_len-timestep: , :]\n",
        "\n",
        "    train_data_x = train_data[0: training_data_len, :]\n",
        "    train_data_y = train_data[0: training_data_len, 0:1]\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(timestep, len(train_data_x)):\n",
        "        x_train.append(train_data_x[i-timestep:i])\n",
        "        y_train.append(train_data_y[i][0])\n",
        "\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
        "\n",
        "    test_data = scaled_data[training_data_len-timestep: , : ]\n",
        "    x_test = []\n",
        "    if (len(data_values.shape) == 1):\n",
        "        y_test = data_values[training_data_len:]\n",
        "    else:\n",
        "        y_test = data_values[training_data_len: , 0]\n",
        "\n",
        "    for i in range(timestep, len(test_data)):\n",
        "        x_test.append(test_data[i-timestep:i])\n",
        "\n",
        "    x_test = np.array(x_test)\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
        "\n",
        "    return x_train, y_train, x_test, y_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TrainModel(x_data, y_data, epoch):\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.LSTM(100, return_sequences=True, input_shape=(x_data.shape[1], x_data.shape[2])))\n",
        "    model.add(layers.LSTM(100, return_sequences=False))\n",
        "    model.add(layers.Dense(25))\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
        "    history = model.fit(x_data, y_data, batch_size= 1, epochs=epoch, validation_split=0.2)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PlotTrainingMetrics(history):\n",
        "    history_data = history.history\n",
        "    \n",
        "    loss_values = history_data['loss']\n",
        "    val_loss_values = history_data['val_loss']\n",
        "    mae_values = history_data['mae']\n",
        "    val_mae_values = history_data['val_mae']\n",
        "    rmse_values = np.sqrt(history_data['loss'])\n",
        "    val_rmse_values = np.sqrt(history_data['val_loss'])\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    gs = fig.add_gridspec(1, 3, wspace=0)\n",
        "    (ax1, ax2, ax3) = gs.subplots(sharey=True)\n",
        "    fig.suptitle('Training and validation metrics')\n",
        "    fig.supxlabel('epochs')\n",
        "\n",
        "    ax1.plot(epochs, mae_values, color = 'blue', label='Training MAE')\n",
        "    ax1.plot(epochs, val_mae_values, color='red', label='Validation MAE')\n",
        "    ax1.set_title('MAE')\n",
        "    ax1.set_xticks(epochs)\n",
        "    ax1.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax1.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax1.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax1.yaxis.grid(True, alpha=0.5)\n",
        "    ax1.set_ylabel('value')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(epochs, loss_values, color = 'blue', label='Training loss')\n",
        "    ax2.plot(epochs, val_loss_values, color='red', label='Validation loss')\n",
        "    ax2.set_title('Loss (MSE)')\n",
        "    ax2.set_xticks(epochs)\n",
        "    ax2.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax2.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax2.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax2.yaxis.grid(True, alpha=0.5)\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3.plot(epochs, rmse_values, color = 'blue', label='Training RMSE')\n",
        "    ax3.plot(epochs, val_rmse_values, color='red', label='Validation RMSE')\n",
        "    ax3.set_title('RMSE')\n",
        "    ax3.set_xticks(epochs)\n",
        "    ax3.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax3.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax3.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax3.yaxis.grid(True, alpha=0.5)\n",
        "    ax3.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PlotPredictions(data, prediction):\n",
        "    train_plot = data[:len(data)-len(prediction)]\n",
        "    validation_plot = data[len(data)-len(prediction):].copy()\n",
        "    validation_plot['Predictions'] = prediction\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_title('Model and Predictions')\n",
        "    ax.set_ylabel('Closing price (USD)')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.plot(train_plot)\n",
        "    ax.plot(validation_plot[['Close', 'Predictions']])\n",
        "    ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "    ax.xaxis.set_minor_locator(dt.MonthLocator(bymonthday=15))\n",
        "    ax.xaxis.set_major_formatter(dt.DateFormatter('%b'))\n",
        "    ax.grid(alpha=0.5, which='both')\n",
        "    ax.legend(['Train', 'Validation', 'Predictions'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiks1f82oTPX"
      },
      "source": [
        "### LSTM without Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdunwbHQR4lK"
      },
      "outputs": [],
      "source": [
        "close_prices = dataset_df['Close']\n",
        "stock_values = close_prices.values\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "stock_data_scaled = scaler.fit_transform(stock_values.reshape(-1,1))\n",
        "\n",
        "train_lstm_only, test_lstm_only = SplitData(stock_data_scaled, train_portion, timestep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFkzCL5ueCUD"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(timestep, len(train_lstm_only)):\n",
        "    x_train.append(train_lstm_only[i-timestep:i, 0])\n",
        "    y_train.append(train_lstm_only[i, 0])\n",
        "\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRltHWv6ga8"
      },
      "outputs": [],
      "source": [
        "x_test = []\n",
        "y_test = stock_values[len(train_lstm_only):]\n",
        "\n",
        "for i in range(timestep, len(test_lstm_only)):\n",
        "  x_test.append(test_lstm_only[i-timestep:i, 0])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel, history = TrainModel(x_train, y_train, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdz6jcJNSTSw",
        "outputId": "00e7ad9d-01a5-4328-bf75-a65be7df38ea"
      },
      "outputs": [],
      "source": [
        "predictions = singleModel.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_test))\n",
        "mse = np.mean(predictions - y_test)**2\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "mae, mse, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotTrainingMetrics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotPredictions(dataset_df.filter(['Close']), predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojots9FfJU2a"
      },
      "source": [
        "### LSTM with User Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2OrcJx0VM4X"
      },
      "outputs": [],
      "source": [
        "combined_data = dataset_df[['Close', 'Compound']].copy()\n",
        "combined_values = combined_data.values\n",
        "\n",
        "close_prices = dataset_df['Close']\n",
        "stock_values = close_prices.values\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60\n",
        "\n",
        "training_data_len = math.ceil(len(combined_values)* train_portion)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "combined_data_scaled_x = scaler.fit_transform(combined_values)\n",
        "combined_data_scaled_y = scaler.fit_transform(stock_values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbKcnfPNKCy"
      },
      "outputs": [],
      "source": [
        "train_data_x = combined_data_scaled_x[0: training_data_len, :]\n",
        "train_data_y = combined_data_scaled_y[0: training_data_len, :]\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(timestep, len(train_data_x)):\n",
        "    x_train.append(train_data_x[i-timestep:i])\n",
        "    y_train.append(train_data_y[i][0])\n",
        "\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xUtgW0iQV00"
      },
      "outputs": [],
      "source": [
        "test_data = combined_data_scaled_x[training_data_len-timestep: , : ]\n",
        "x_test = []\n",
        "y_test = stock_values[training_data_len:]\n",
        "\n",
        "for i in range(timestep, len(test_data)):\n",
        "  x_test.append(test_data[i-timestep:i])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentimentModel, history_sentiment = TrainModel(x_train, y_train, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heZho7PMSEu9",
        "outputId": "7ea48ef2-293c-49f4-ab00-1d353583adb3"
      },
      "outputs": [],
      "source": [
        "predictions = sentimentModel.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_test))\n",
        "mse = np.mean(predictions - y_test)**2\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "mae, mse, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotTrainingMetrics(history_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotPredictions(dataset_df.filter(['Close']), predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "close_prices = dataset_df['Close']\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60\n",
        "\n",
        "a, b, c, d, s = SplitDataNew(close_prices, train_portion, timestep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentimentModel, history_sentiment = TrainModel(a, b, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dictions = sentimentModel.predict(c)\n",
        "\n",
        "dictions = s.inverse_transform(dictions)\n",
        "\n",
        "mae = np.mean(np.abs(dictions - d))\n",
        "mse = np.mean(dictions - d)**2\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((d - dictions)/d)) * 100\n",
        "mae, mse, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotTrainingMetrics(history_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotPredictions(dataset_df.filter(['Close']), dictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
