{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementasi Metode VADER-LSTM dalam Pengujian Pengaruh Sentimen Investor terhadap Prediksi Harga Saham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljaywiikl4zP",
        "outputId": "49b92a99-8e67-4e03-e2b4-4c7eb7f33112"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import nltk\n",
        "import unicodedata\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.dates as dt\n",
        "from matplotlib.dates import DateFormatter\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras import models as md\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoB1vjA469iM"
      },
      "outputs": [],
      "source": [
        "company = \"TSLA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Tweet Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "s4gocCIkkUkY",
        "outputId": "e5a6b992-2edf-4d84-d7ee-5fa4f03d4a2f"
      },
      "outputs": [],
      "source": [
        "all_tweets = pd.read_csv(\"stock_tweets.csv\")\n",
        "print(all_tweets.shape)\n",
        "all_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Hm4AXNZCk2el",
        "outputId": "319b5eda-637b-4401-be8a-d5843f2fb11b"
      },
      "outputs": [],
      "source": [
        "tweet_df = all_tweets[all_tweets['Stock Name'] == company]\n",
        "tweet_df = tweet_df.drop(['Company Name', 'Stock Name'], axis=1)\n",
        "tweet_df['Date'] = pd.to_datetime(tweet_df['Date']).dt.date\n",
        "tweet_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pre-processing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df = tweet_df.copy()\n",
        "processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    '''\n",
        "    Takes a tweet as an input and output the list of tokens.\n",
        "    '''\n",
        "    \n",
        "    import emoji\n",
        "    import re\n",
        "    from nltk import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer\n",
        "    \n",
        "    # Initialization\n",
        "    new_tweet = tweet\n",
        "    \n",
        "    ## Changes on string\n",
        "    \n",
        "    # Remove urls\n",
        "    new_tweet = re.sub(r'https?://[^ ]+', '', new_tweet)\n",
        "    \n",
        "    # Remove usernames\n",
        "    new_tweet = re.sub(r'@[^ ]+', '', new_tweet)\n",
        "    \n",
        "    # Remove hashtags\n",
        "    new_tweet = re.sub(r'#', '', new_tweet)\n",
        "    \n",
        "    # Character normalization\n",
        "    new_tweet = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', new_tweet)\n",
        "    \n",
        "    # Emoji transformation\n",
        "    new_tweet = emoji.demojize(new_tweet)\n",
        "    \n",
        "    # Punctuation and special characters\n",
        "    new_tweet = re.sub(r' 0 ', 'zero', new_tweet)\n",
        "    new_tweet = re.sub(r'[^A-Za-z ]', '', new_tweet)\n",
        "    \n",
        "    # Lower casing\n",
        "    new_tweet = new_tweet.lower()\n",
        "    \n",
        "    \n",
        "    ## Changes on tokens\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(new_tweet)\n",
        "    \n",
        "    porter = PorterStemmer()\n",
        "\n",
        "    for token in tokens:\n",
        "        # Stopwords removal\n",
        "        if token in stopwords.words('english'):\n",
        "            tokens.remove(token)\n",
        "        # Stemming\n",
        "        token = porter.stem(token)\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "    # return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for indx, row in processed_df.T.items():\n",
        "    try:\n",
        "        processed_df.at[indx, 'Tweet'] = preprocess_tweet(processed_df.at[indx, 'Tweet'])\n",
        "    except TypeError:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.to_csv('tweet_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Labeling the tweets with VADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df = pd.read_csv('tweet_processed.csv')\n",
        "processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
        "processed_df['Tweet'] = processed_df['Tweet'].astype(str)\n",
        "processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
        "processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "for indx, row in processed_df.T.items():\n",
        "    try:\n",
        "        sentence_sentiment = sentiment_analyzer.polarity_scores(processed_df.loc[indx, 'Tweet'])\n",
        "        processed_df.at[indx, 'Negative'] = sentence_sentiment['neg']\n",
        "        processed_df.at[indx, 'Neutral'] = sentence_sentiment['neu']\n",
        "        processed_df.at[indx, 'Positive'] = sentence_sentiment['pos']\n",
        "        processed_df.at[indx, 'Compound'] = sentence_sentiment['compound']\n",
        "    except TypeError:\n",
        "        print (processed_df.loc[indx, 'Tweet'])\n",
        "        print (indx)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.to_csv('tweet_processed_labeled.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df = pd.read_csv('tweet_processed_labeled.csv')\n",
        "processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
        "processed_df['Date'] = processed_df['Date'].dt.date\n",
        "processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_tweets_df = processed_df.copy()\n",
        "\n",
        "for indx, row in processed_df.T.items():\n",
        "    if (processed_df.at[indx, 'Compound'] > 0.5):\n",
        "        labeled_tweets_df.at[indx, 'Sentiment'] = 'Positive'\n",
        "    elif (processed_df.at[indx, 'Compound'] < -0.5):\n",
        "        labeled_tweets_df.at[indx, 'Sentiment'] = 'Negative'\n",
        "    else:\n",
        "        labeled_tweets_df.at[indx, 'Sentiment'] = 'Neutral'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "bar_colors = ['grey', 'green', 'red']\n",
        "ax.bar(['Neutral', 'Positive', 'Negative'], labeled_tweets_df['Sentiment'].value_counts(), color=bar_colors)\n",
        "ax.set(xlabel=\"Sentimen\", ylabel=\"Count\", title=f\"{company} Sentiment Result\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily_sentiments_df = labeled_tweets_df.groupby([labeled_tweets_df['Date']]).mean(numeric_only=True)\n",
        "daily_sentiments_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(daily_sentiments_df['Compound'], color='#008B8B')\n",
        "ax.set(xlabel=\"Date\", ylabel=\"Sentiment\", title=f\"{company} Daily Sentiment\")\n",
        "ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Grouping sentiments by day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "gjvqS1Ijw7H3",
        "outputId": "f5a4c12e-03f7-4550-ba6f-4d5de8f1aa5f"
      },
      "outputs": [],
      "source": [
        "daily_sentiments_df = processed_df.groupby([processed_df['Date']]).mean(numeric_only=True)\n",
        "daily_sentiments_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "V8kKkLSulEMe",
        "outputId": "3fb9f070-4d05-41c7-f132-3395e3c37618"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(daily_sentiments_df['Compound'], color='#008B8B')\n",
        "ax.set(xlabel=\"Date\", ylabel=\"Score\", title=f\"{company} Daily Sentiment\")\n",
        "ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4S5DD9P95z9x",
        "outputId": "c3b7c868-417f-4961-de3a-33c060ab4e5d"
      },
      "outputs": [],
      "source": [
        "all_stocks = pd.read_csv(\"stock_yfinance_data.csv\")\n",
        "all_stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "L72o-YzF7zw7",
        "outputId": "675f77de-f30c-4760-b7d6-c09e08520ce8"
      },
      "outputs": [],
      "source": [
        "stock_df = all_stocks[all_stocks['Stock Name'] == company]\n",
        "stock_df = stock_df.drop('Stock Name', axis=1)\n",
        "stock_df['Date'] = pd.to_datetime(stock_df['Date']).dt.date\n",
        "stock_df = stock_df.set_index(\"Date\")\n",
        "stock_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stock_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "K0pNKNfZCCKS",
        "outputId": "48e7c276-e5da-4e91-e616-627d072c8c12"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.plot(stock_df['Close'], color='#008B8B')\n",
        "ax.set(xlabel=\"Date\", ylabel=\"USD\", title=f\"{company} Stock Price\")\n",
        "ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "QNbSzCPxogZP",
        "outputId": "a298b6b6-a8c8-4d01-f091-090cf6fbe619"
      },
      "outputs": [],
      "source": [
        "dataset_df = stock_df.copy()\n",
        "dataset_df = dataset_df.join(daily_sentiments_df, how=\"left\", on=\"Date\")\n",
        "dataset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(dataset_df[['Close', 'Compound']].values)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(scaled_data, label=[\"Closing Price\", 'Sentiment'])\n",
        "ax.set(xlabel=\"Date\", ylabel=\"Scaled Data\", title=f\"{company} Scaled Price Overlaid with Daily Sentiment\")\n",
        "ax.legend()\n",
        "# ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SplitData(data, train_size, timestep):\n",
        "    data_values = data.values\n",
        "    training_data_len = math.ceil(len(data)* train_size)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    if (len(data_values.shape) == 1):\n",
        "        scaled_data = scaler.fit_transform(data_values.reshape(-1,1))\n",
        "    else:\n",
        "        scaled_data = scaler.fit_transform(data_values)\n",
        "        scaled_index = scaler.fit_transform(data_values[:, 0:1].flatten().reshape(-1,1))\n",
        "\n",
        "    train_data = scaled_data[0: training_data_len, :]\n",
        "    test_data = scaled_data[training_data_len-timestep: , :]\n",
        "\n",
        "    train_data_x = train_data[0: training_data_len, :]\n",
        "    train_data_y = train_data[0: training_data_len, 0:1]\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(timestep, len(train_data_x)):\n",
        "        x_train.append(train_data_x[i-timestep:i])\n",
        "        y_train.append(train_data_y[i][0])\n",
        "\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
        "\n",
        "    test_data = scaled_data[training_data_len-timestep: , : ]\n",
        "    x_test = []\n",
        "    if (len(data_values.shape) == 1):\n",
        "        y_test = data_values[training_data_len:]\n",
        "    else:\n",
        "        y_test = data_values[training_data_len: , 0]\n",
        "\n",
        "    for i in range(timestep, len(test_data)):\n",
        "        x_test.append(test_data[i-timestep:i])\n",
        "\n",
        "    x_test = np.array(x_test)\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
        "\n",
        "    return x_train, y_train, x_test, y_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TrainModel(x_data, y_data, epoch):\n",
        "\n",
        "    regressor = Sequential()\n",
        "    regressor.add(LSTM(units=64, return_sequences=True, input_shape=(x_data.shape[1], x_data.shape[2])))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    regressor.add(LSTM(units=64, return_sequences=False))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    regressor.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
        "    history = regressor.fit(x_data, y_data, batch_size=1, epochs=epoch, validation_split=0.2)\n",
        "\n",
        "    return regressor, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PlotTrainingMetrics(history):\n",
        "    history_data = history.history\n",
        "    \n",
        "    loss_values = history_data['loss']\n",
        "    val_loss_values = history_data['val_loss']\n",
        "    mae_values = history_data['mae']\n",
        "    val_mae_values = history_data['val_mae']\n",
        "    rmse_values = np.sqrt(history_data['loss'])\n",
        "    val_rmse_values = np.sqrt(history_data['val_loss'])\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    gs = fig.add_gridspec(1, 3, wspace=0)\n",
        "    (ax1, ax2, ax3) = gs.subplots(sharey=True)\n",
        "    fig.suptitle('Training and validation metrics')\n",
        "    fig.supxlabel('epochs')\n",
        "\n",
        "    ax1.plot(epochs, mae_values, color = 'blue', label='Training MAE')\n",
        "    ax1.plot(epochs, val_mae_values, color='red', label='Validation MAE')\n",
        "    ax1.set_title('MAE')\n",
        "    ax1.set_xticks(epochs)\n",
        "    ax1.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax1.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax1.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax1.yaxis.grid(True, alpha=0.5)\n",
        "    ax1.set_ylabel('value')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(epochs, loss_values, color = 'blue', label='Training loss')\n",
        "    ax2.plot(epochs, val_loss_values, color='red', label='Validation loss')\n",
        "    ax2.set_title('Loss (MSE)')\n",
        "    ax2.set_xticks(epochs)\n",
        "    ax2.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax2.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax2.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax2.yaxis.grid(True, alpha=0.5)\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3.plot(epochs, rmse_values, color = 'blue', label='Training RMSE')\n",
        "    ax3.plot(epochs, val_rmse_values, color='red', label='Validation RMSE')\n",
        "    ax3.set_title('RMSE')\n",
        "    ax3.set_xticks(epochs)\n",
        "    ax3.xaxis.set_major_locator(ticker.MultipleLocator(len(loss_values)/5))\n",
        "    ax3.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    ax3.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax3.yaxis.grid(True, alpha=0.5)\n",
        "    ax3.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PlotPredictions(data, prediction):\n",
        "    train_plot = data[:len(data)-len(prediction)]\n",
        "    validation_plot = data[len(data)-len(prediction):].copy()\n",
        "    validation_plot['Predictions'] = prediction\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_title('Model and Predictions')\n",
        "    ax.set_ylabel('Closing price (USD)')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.plot(train_plot)\n",
        "    ax.plot(validation_plot[['Close', 'Predictions']])\n",
        "    ax.xaxis.set_major_locator(dt.MonthLocator())\n",
        "    ax.xaxis.set_minor_locator(dt.MonthLocator(bymonthday=15))\n",
        "    ax.xaxis.set_major_formatter(dt.DateFormatter('%b'))\n",
        "    ax.grid(alpha=0.5, which='both')\n",
        "    ax.legend(['Train', 'Validation', 'Predictions'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiks1f82oTPX"
      },
      "source": [
        "### LSTM without Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdunwbHQR4lK"
      },
      "outputs": [],
      "source": [
        "close_prices = dataset_df['Close']\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60\n",
        "\n",
        "\n",
        "x_train, y_train, x_test, y_test, scaler = SplitData(close_prices, train_portion, timestep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel, history_single = TrainModel(x_train, y_train, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel.save('saved_model/single-features.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel = md.load_model('saved_model/single-features.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel.get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "singleModel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdz6jcJNSTSw",
        "outputId": "00e7ad9d-01a5-4328-bf75-a65be7df38ea"
      },
      "outputs": [],
      "source": [
        "predictions_single = singleModel.predict(x_test)\n",
        "predictions_single = scaler.inverse_transform(predictions_single)\n",
        "\n",
        "mae = np.mean(np.abs(predictions_single - y_test))\n",
        "mse = np.mean((predictions_single - y_test)**2)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((y_test - predictions_single)/y_test)) * 100\n",
        "mae, mse, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotTrainingMetrics(history_single)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotPredictions(dataset_df.filter(['Close']), predictions_single)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojots9FfJU2a"
      },
      "source": [
        "### LSTM with User Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2OrcJx0VM4X"
      },
      "outputs": [],
      "source": [
        "combined_data = dataset_df[['Close', 'Compound']].copy()\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60\n",
        "\n",
        "x_train, y_train, x_test, y_test, scaler = SplitData(combined_data, train_portion, timestep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentimentModel, history_sentiment = TrainModel(x_train, y_train, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentimentModel.save('saved_model/multi-features.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentimentModel = md.load_model('saved_model/multi-features.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = sentimentModel.predict(x_test)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = scaler.inverse_transform(predictions)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mae = np.mean(np.abs(predictions - y_test))\n",
        "mse = np.mean((predictions - y_test)**2)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "mae, mse, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotTrainingMetrics(history_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PlotPredictions(dataset_df.filter(['Close']), predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TrainModelFineTune(x_data, y_data, epoch, neuron_units):\n",
        "\n",
        "    regressor = Sequential()\n",
        "    regressor.add(LSTM(units=neuron_units, return_sequences=True, input_shape=(x_data.shape[1], x_data.shape[2])))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    regressor.add(LSTM(units=neuron_units, return_sequences=False))\n",
        "    regressor.add(Dropout(0.2))\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    regressor.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "    history = regressor.fit(x_data, y_data, batch_size=1, epochs=epoch, validation_split=0.2)\n",
        "\n",
        "    return regressor, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tune Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = [5, 10, 20, 30, 40, 50]\n",
        "\n",
        "epoch_finetune_history = []\n",
        "epoch_finetune_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_data = dataset_df[['Close', 'Compound']].copy()\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(epochs)):\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_title('Loss value for ' + str(epochs[i]) + ' epochs')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('error value')\n",
        "    ax.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax.yaxis.grid(True, alpha=0.5)\n",
        "\n",
        "    for j in range(10):\n",
        "\n",
        "        x_train, y_train, x_test, y_test, scaler = SplitData(combined_data, train_portion, timestep)\n",
        "\n",
        "        start_time = time.time()\n",
        "        sentimentModel, history = TrainModelFineTune(x_train, y_train, epochs[i], 32)\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "\n",
        "        predictions = sentimentModel.predict(x_test)\n",
        "        predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "        mae = np.mean(np.abs(predictions - y_test))\n",
        "        mse = np.mean((predictions - y_test)**2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "\n",
        "        epoch_finetune_test.append([epochs[i], j, mae, mse, rmse, mape, training_time])\n",
        "\n",
        "        history_data = history.history\n",
        "    \n",
        "        loss_values = history_data['loss']\n",
        "        val_loss_values = history_data['val_loss']\n",
        "\n",
        "        epoch_finetune_history.append([epochs[i], j, history_data['loss'], history_data['val_loss'], history_data['mae'], history_data['val_mae'], training_time])\n",
        "\n",
        "        ax.plot(loss_values, color = 'blue')\n",
        "        ax.plot(val_loss_values, color='red')\n",
        "\n",
        "    plt.savefig('epoch_finetune_data/'+ str(epochs[i]) +'_epochs_diagnostic.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_test_finetuning = pd.DataFrame(epoch_finetune_test, columns=['#epoch', '#try', 'MAE', 'MSE', 'RMSE', 'MAPE', 'train_time'])\n",
        "epoch_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_test_finetuning.to_csv('epoch_finetune_data/finetuning_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_test_finetuning = pd.read_csv('epoch_finetune_data/finetuning_test_data.csv')\n",
        "epoch_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch5 = epoch_test_finetuning[50:60]\n",
        "epoch5.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_test_finetune_rmse = epoch_test_finetuning.pivot(index='#try', columns='#epoch', values='RMSE')\n",
        "epoch_test_finetune_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "ax.boxplot(epoch_test_finetune_rmse)\n",
        "ax.set_xticklabels(['5', '10', '20', '30', '40', '50'])\n",
        "ax.set_title(\"Boxplot Hasil Fine Tuning Parameter Epoch\")\n",
        "ax.set_xlabel('Number of epochs')\n",
        "ax.set_ylabel('RMSE Values (lower is better)')\n",
        "plt.savefig('epoch_finetune_data/epoch_boxplot.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_history_finetuning = pd.DataFrame(epoch_finetune_history, columns=['#epoch', '#try', 'loss', 'val_loss', 'mae', 'val_mae', 'train_time'])\n",
        "epoch_history_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_history_finetuning.to_csv('neuron_finetune_data/finetuning_history_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neuron Number Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_final = 10\n",
        "neurons = [16, 32, 64, 128]\n",
        "\n",
        "neuron_finetune_history = []\n",
        "neuron_finetune_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_data = dataset_df[['Close', 'Compound']].copy()\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(neurons)):\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_title('Loss value for ' + str(neurons[i]) + ' neurons')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('error value')\n",
        "    ax.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax.yaxis.grid(True, alpha=0.5)\n",
        "\n",
        "    for j in range(20):\n",
        "\n",
        "        x_train, y_train, x_test, y_test, scaler = SplitData(combined_data, train_portion, timestep)\n",
        "\n",
        "        start_time = time.time()\n",
        "        sentimentModel, history = TrainModelFineTune(x_train, y_train, epoch_final, neurons[i])\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "\n",
        "        predictions = sentimentModel.predict(x_test)\n",
        "        predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "        mae = np.mean(np.abs(predictions - y_test))\n",
        "        mse = np.mean((predictions - y_test)**2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "\n",
        "        neuron_finetune_test.append([neurons[i], j, mae, mse, rmse, mape, training_time])\n",
        "\n",
        "        history_data = history.history\n",
        "    \n",
        "        loss_values = history_data['loss']\n",
        "        val_loss_values = history_data['val_loss']\n",
        "\n",
        "        neuron_finetune_history.append([neurons[i], j, history_data['loss'], history_data['val_loss'], history_data['mae'], history_data['val_mae'], training_time])\n",
        "\n",
        "        ax.plot(loss_values, color = 'blue')\n",
        "        ax.plot(val_loss_values, color='red')\n",
        "\n",
        "    plt.savefig('neuron_finetune_data/'+ str(neurons[i]) +'_neuron_diagnostic.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_test_finetuning = pd.DataFrame(neuron_finetune_test, columns=['#neuron', '#try', 'MAE', 'MSE', 'RMSE', 'MAPE', 'train_time'])\n",
        "neuron_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_test_finetuning.to_csv('neuron_finetune_data/finetuning_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_test_finetuning = pd.read_csv('neuron_finetune_data/finetuning_test_data.csv')\n",
        "neuron_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = neuron_test_finetuning[60:80]\n",
        "y.drop(60).drop(67).drop(75).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_test_finetune_rmse = neuron_test_finetuning.pivot(index='#try', columns='#neuron', values='RMSE')\n",
        "neuron_test_finetune_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "ax.boxplot(neuron_test_finetune_rmse)\n",
        "ax.set_xticklabels(['16', '32', '64', '128'])\n",
        "ax.set_title(\"Boxplot Hasil Fine Tuning Parameter LSTM Unit\")\n",
        "ax.set_xlabel('Number of neurons')\n",
        "ax.set_ylabel('RMSE Values (lower is better)')\n",
        "plt.savefig('neuron_finetune_data/neuron_boxplot.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_history_finetuning = pd.DataFrame(neuron_finetune_history, columns=['#neuron', '#try', 'loss', 'val_loss', 'mae', 'val_mae', 'train_time'])\n",
        "neuron_history_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_history_finetuning.to_csv('neuron_finetune_data/finetuning_history_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuron_history_finetuning = pd.read_csv('neuron_finetune_data/finetuning_history_data.csv')\n",
        "neuron_history_finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPARE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mode = ['single', 'combined']\n",
        "\n",
        "compare_finetune_history = []\n",
        "compare_finetune_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_data = dataset_df['Close']\n",
        "combined_data = dataset_df[['Close', 'Compound']].copy()\n",
        "\n",
        "train_portion = 0.8\n",
        "timestep = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(model_mode)):\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_title('Loss value for ' + str(model_mode[i]) + ' mode')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('error value')\n",
        "    ax.xaxis.grid(True, which='both', alpha=0.5)\n",
        "    ax.yaxis.grid(True, alpha=0.5)\n",
        "\n",
        "    for j in range(30):\n",
        "        \n",
        "        if (model_mode == 'single'):\n",
        "            x_train, y_train, x_test, y_test, scaler = SplitData(single_data, train_portion, timestep)\n",
        "        else:\n",
        "            x_train, y_train, x_test, y_test, scaler = SplitData(combined_data, train_portion, timestep)\n",
        "\n",
        "        start_time = time.time()\n",
        "        sentimentModel, history = TrainModelFineTune(x_train, y_train, 10, 64)\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "\n",
        "        predictions = sentimentModel.predict(x_test)\n",
        "        predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "        mae = np.mean(np.abs(predictions - y_test))\n",
        "        mse = np.mean((predictions - y_test)**2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - predictions)/y_test)) * 100\n",
        "\n",
        "        compare_finetune_test.append([model_mode[i], j, mae, mse, rmse, mape, training_time])\n",
        "\n",
        "        history_data = history.history\n",
        "    \n",
        "        loss_values = history_data['loss']\n",
        "        val_loss_values = history_data['val_loss']\n",
        "\n",
        "        compare_finetune_history.append([model_mode[i], j, history_data['loss'], history_data['val_loss'], history_data['mae'], history_data['val_mae'], training_time])\n",
        "\n",
        "        ax.plot(loss_values, color = 'blue')\n",
        "        ax.plot(val_loss_values, color='red')\n",
        "\n",
        "    plt.savefig('compare_finetune_data/'+ str(model_mode[i]) +'_mode_diagnostic.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_test_finetuning = pd.DataFrame(compare_finetune_test, columns=['mode', '#try', 'MAE', 'MSE', 'RMSE', 'MAPE', 'train_time'])\n",
        "compare_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_test_finetuning.to_csv('compare_finetune_data/finetuning_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_test_finetuning = pd.read_csv('compare_finetune_data/finetuning_test_data.csv')\n",
        "compare_test_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz = compare_test_finetuning[30:60]\n",
        "xyz.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_test_finetune_rmse = compare_test_finetuning.pivot(index='#try', columns='mode', values='RMSE')\n",
        "compare_test_finetune_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "ax.boxplot(compare_test_finetune_rmse)\n",
        "ax.set_xticklabels(['Combined Dataset', 'Single Dataset'])\n",
        "ax.set_title(\"Perbandingan Model untuk Combined Dataset vs Single Dataset\")\n",
        "ax.set_ylabel('RMSE Values (lower is better)')\n",
        "ax.grid()\n",
        "plt.savefig('compare_finetune_data/comparison_boxplot.png', facecolor=(1, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_history_finetuning = pd.DataFrame(compare_finetune_history, columns=['mode', '#try', 'loss', 'val_loss', 'mae', 'val_mae', 'train_time'])\n",
        "compare_history_finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_history_finetuning.to_csv('compare_finetune_data/finetuning_history_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_history_finetuning = pd.read_csv('compare_finetune_data/finetuning_history_data.csv')\n",
        "compare_history_finetuning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
